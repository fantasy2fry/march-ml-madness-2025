{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "296b7508-9f4b-448d-a1f2-bef77e310f54",
   "metadata": {},
   "source": [
    "# First model regular season games approach| Sebislaw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae40407-1d9b-4be5-b65f-d1102671330b",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0f0e2c-ff45-40de-8b1e-d3b64364a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path  import join\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6a0df-73e5-4ab1-bdac-5eb60b1d072b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f89e969-c878-4c25-83fa-1a1b6a64bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '..\\\\..\\\\data'\n",
    "\n",
    "# The Basics ------------------------------------------------------------------------\n",
    "# Men\n",
    "MTeams = pd.read_csv(join(data_path, 'MTeams.csv'))\n",
    "MSeasons = pd.read_csv(join(data_path, 'MSeasons.csv'))\n",
    "MNCAATourneySeeds = pd.read_csv(join(data_path, 'MNCAATourneySeeds.csv'))\n",
    "MRegularSeasonCompactResults = pd.read_csv(join(data_path, 'MRegularSeasonCompactResults.csv'))\n",
    "MNCAATourneyCompactResults = pd.read_csv(join(data_path, 'MNCAATourneyCompactResults.csv'))\n",
    "# Women\n",
    "WTeams = pd.read_csv(join(data_path, 'WTeams.csv'))\n",
    "WSeasons = pd.read_csv(join(data_path, 'WSeasons.csv'))\n",
    "WNCAATourneySeeds = pd.read_csv(join(data_path, 'WNCAATourneySeeds.csv'))\n",
    "WRegularSeasonCompactResults = pd.read_csv(join(data_path, 'WRegularSeasonCompactResults.csv'))\n",
    "WNCAATourneyCompactResults = pd.read_csv(join(data_path, 'WNCAATourneyCompactResults.csv'))\n",
    "# Other\n",
    "SampleSubmissionStage1 = pd.read_csv(join(data_path, 'SampleSubmissionStage1.csv'))\n",
    "SampleSubmissionStage2 = pd.read_csv(join(data_path, 'SampleSubmissionStage2.csv'))\n",
    "SeedBenchmarkStage1 = pd.read_csv(join(data_path, 'SeedBenchmarkStage1.csv'))\n",
    "\n",
    "# Team Box Scores ------------------------------------------------------------------------\n",
    "# Men\n",
    "MRegularSeasonDetailedResults = pd.read_csv(join(data_path, 'MRegularSeasonDetailedResults.csv'))\n",
    "MNCAATourneyDetailedResults = pd.read_csv(join(data_path, 'MNCAATourneyDetailedResults.csv'))\n",
    "# Women\n",
    "WRegularSeasonDetailedResults = pd.read_csv(join(data_path, 'WRegularSeasonDetailedResults.csv'))\n",
    "WNCAATourneyDetailedResults = pd.read_csv(join(data_path, 'WNCAATourneyDetailedResults.csv'))\n",
    "\n",
    "# Geography ------------------------------------------------------------------------\n",
    "# All\n",
    "Cities = pd.read_csv(join(data_path, 'Cities.csv'))\n",
    "Conferences = pd.read_csv(join(data_path, 'Conferences.csv'))\n",
    "# Men\n",
    "MGameCities = pd.read_csv(join(data_path, 'MGameCities.csv'))\n",
    "# Women\n",
    "WGameCities = pd.read_csv(join(data_path, 'WGameCities.csv'))\n",
    "\n",
    "# Public Rankings ------------------------------------------------------------------------\n",
    "# Men\n",
    "MMasseyOrdinals = pd.read_csv(join(data_path, 'MMasseyOrdinals.csv')) # men only\n",
    "\n",
    "# Supplements ------------------------------------------------------------------------\n",
    "# Men\n",
    "MTeamCoaches = pd.read_csv(join(data_path, 'MTeamCoaches.csv')) # men only\n",
    "MTeamConferences = pd.read_csv(join(data_path, 'MTeamConferences.csv'))\n",
    "MConferenceTourneyGames = pd.read_csv(join(data_path, 'MConferenceTourneyGames.csv'))\n",
    "MSecondaryTourneyTeams = pd.read_csv(join(data_path, 'MSecondaryTourneyTeams.csv'))\n",
    "MSecondaryTourneyCompactResults = pd.read_csv(join(data_path, 'MSecondaryTourneyCompactResults.csv'))\n",
    "MTeamSpellings = pd.read_csv(join(data_path, \"MTeamSpellings.csv\"), encoding='cp1252')\n",
    "MNCAATourneySlots = pd.read_csv(join(data_path, 'MNCAATourneySlots.csv'))\n",
    "MNCAATourneySeedRoundSlots = pd.read_csv(join(data_path, 'MNCAATourneySeedRoundSlots.csv')) # men only\n",
    "# Women\n",
    "WTeamConferences = pd.read_csv(join(data_path, 'WTeamConferences.csv'))\n",
    "WConferenceTourneyGames = pd.read_csv(join(data_path, 'WConferenceTourneyGames.csv'))\n",
    "WSecondaryTourneyTeams = pd.read_csv(join(data_path, 'WSecondaryTourneyTeams.csv'))\n",
    "WSecondaryTourneyCompactResults = pd.read_csv(join(data_path, 'WSecondaryTourneyCompactResults.csv'))\n",
    "WTeamSpellings = pd.read_csv(join(data_path, 'WTeamSpellings.csv'), encoding='cp1252')\n",
    "WNCAATourneySlots = pd.read_csv(join(data_path, 'WNCAATourneySlots.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc3f37-451a-46e8-957a-c1ddba4d444c",
   "metadata": {},
   "source": [
    "## First models based on data from past games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a6a902-9338-4e92-b060-bd95d418a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(GameReslts, Conferences, include_conferences, seasons):\n",
    "    # Make a copy so that the original GameReslts is not modified.\n",
    "    df = GameReslts.copy()\n",
    "    \n",
    "    if include_conferences:\n",
    "        # =========== MERGE CONFERENCES =========== #\n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            Conferences[['Season', 'TeamID', 'ConfAbbrev']],\n",
    "            how='left',\n",
    "            left_on=['Season', 'WTeamID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        ).rename(columns={'ConfAbbrev': 'WConf'}).drop(columns='TeamID')\n",
    "        \n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            Conferences[['Season', 'TeamID', 'ConfAbbrev']],\n",
    "            how='left',\n",
    "            left_on=['Season', 'LTeamID'],\n",
    "            right_on=['Season', 'TeamID']\n",
    "        ).rename(columns={'ConfAbbrev': 'LConf'}).drop(columns='TeamID')\n",
    "    \n",
    "    # =========== DEFINE TEAM1/TEAM2 & LABEL =========== #\n",
    "    def define_team1_team2(row):\n",
    "        w, l = row['WTeamID'], row['LTeamID']\n",
    "        return (w, l, 1) if w < l else (l, w, 0)\n",
    "    \n",
    "    df[['Team1', 'Team2', 'label']] = df.apply(define_team1_team2, axis=1, result_type='expand')\n",
    "    \n",
    "    # =========== ONE-HOT WLoc FROM TEAM1's PERSPECTIVE =========== #\n",
    "    def get_team1_loc(row):\n",
    "        wloc = row['WLoc']\n",
    "        lbl = row['label']\n",
    "        if wloc == 'N':\n",
    "            return 'N'\n",
    "        if wloc == 'H':\n",
    "            return 'H' if lbl == 1 else 'A'\n",
    "        if wloc == 'A':\n",
    "            return 'A' if lbl == 1 else 'H'\n",
    "        return 'N'  # fallback\n",
    "    \n",
    "    df['Team1WLoc'] = df.apply(get_team1_loc, axis=1)\n",
    "    df = pd.get_dummies(df, columns=['Team1WLoc'], prefix='Loc', drop_first=False)\n",
    "    \n",
    "    # Ensure that all location columns are present, even if not generated by get_dummies\n",
    "    for col in ['Loc_A', 'Loc_H', 'Loc_N']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    if include_conferences:\n",
    "        # =========== ONE-HOT CONFERENCES =========== #\n",
    "        def get_team1_conf(row):\n",
    "            return row['WConf'] if row['label'] == 1 else row['LConf']\n",
    "        def get_team2_conf(row):\n",
    "            return row['LConf'] if row['label'] == 1 else row['WConf']\n",
    "        \n",
    "        df['Team1Conf'] = df.apply(get_team1_conf, axis=1)\n",
    "        df['Team2Conf'] = df.apply(get_team2_conf, axis=1)\n",
    "        df = pd.get_dummies(df, columns=['Team1Conf', 'Team2Conf'], prefix=['T1Conf', 'T2Conf'], drop_first=False)\n",
    "        \n",
    "        # Ensure that all possible conferences are present in the one-hot encoding.\n",
    "        all_confs = sorted(Conferences['ConfAbbrev'].unique())\n",
    "        for conf in all_confs:\n",
    "            col_name_1 = f\"T1Conf_{conf}\"\n",
    "            col_name_2 = f\"T2Conf_{conf}\"\n",
    "            if col_name_1 not in df.columns:\n",
    "                df[col_name_1] = 0\n",
    "            if col_name_2 not in df.columns:\n",
    "                df[col_name_2] = 0\n",
    "    \n",
    "    # =========== FEATURES & LABEL =========== #\n",
    "    # Preserve WScore and LScore along with the one-hot encoded columns.\n",
    "    feature_cols = [c for c in df.columns if c.startswith('Loc_') \n",
    "                    or c.startswith('T1Conf_') or c.startswith('T2Conf_')\n",
    "                    or c in ['WScore', 'LScore']]\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df['label']\n",
    "    \n",
    "    # Filter by the specified seasons without modifying the original data further.\n",
    "    mask = df['Season'].isin(seasons)\n",
    "    X, y = X[mask], y[mask]\n",
    "    df = df[mask].copy()\n",
    "\n",
    "    # Define the desired order for the feature columns\n",
    "    ordered_feature_columns = ['Loc_A', 'Loc_H', 'Loc_N']\n",
    "    ordered_feature_columns += sorted([col for col in X.columns if col.startswith('T1Conf_')])\n",
    "    ordered_feature_columns += sorted([col for col in X.columns if col.startswith('T2Conf_')])\n",
    "    ordered_feature_columns += ['WScore', 'LScore']\n",
    "    \n",
    "    # Reorder the X DataFrame\n",
    "    X = X[ordered_feature_columns]\n",
    "    \n",
    "    # Optionally, for the full dataframe (df), define a complete column order\n",
    "    ordered_df_columns = ['Season', 'DayNum', 'WTeamID', 'WScore', 'LTeamID', 'LScore', 'label', 'Team1', 'Team2'] + ordered_feature_columns\n",
    "    df = df.reindex(columns=ordered_df_columns)\n",
    "    \n",
    "    return X, y, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1587d3b1-07fd-40ec-92ab-42228f5fd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, max_iter_num=1000):\n",
    "    # =========== TRAIN LOGISTIC REGRESSION =========== #\n",
    "    model = LogisticRegression(max_iter=max_iter_num)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    return model, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e9c2c-273f-4f98-8538-ae617d992bbe",
   "metadata": {},
   "source": [
    "### On compact data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c205739-fc41-4483-8beb-d25d98a6004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seasons_array = [\n",
    "    [i for i in range(2022, 2023)],\n",
    "    [i for i in range(2010, 2023)]\n",
    "]\n",
    "test_seasons_array = [\n",
    "    [2023],\n",
    "    [2023]\n",
    "]\n",
    "\n",
    "def print_tmp(train_seasons, test_seasons, y_test, y_pred_proba):\n",
    "    print(f\"Train seasons: {train_seasons}\") \n",
    "    print(f\"Test seasons: {test_seasons}\") \n",
    "    brier = brier_score_loss(y_test, y_pred_proba)\n",
    "    print(f\"Brier Score: {brier:.10f}\")\n",
    "    y_pred_proba = np.where(\n",
    "        y_pred_proba > 0.95, 1,\n",
    "            np.where(\n",
    "                y_pred_proba < 0.05, 0,\n",
    "                np.where(\n",
    "                    (y_pred_proba >= 0.45) & (y_pred_proba <= 0.55),\n",
    "                    0.5,\n",
    "                    y_pred_proba\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    brier = brier_score_loss(y_test, y_pred_proba)\n",
    "    print(f\"Brier Score after tresholding: {brier:.10f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "349fb512-e630-41fe-8582-a72e813b0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test on regular season\n",
      "Train seasons: [2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2220639657\n",
      "Brier Score after tresholding: 0.2222225503\n",
      "\n",
      "Train on regular season and test on tournament\n",
      "Train seasons: [2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2239554386\n",
      "Brier Score after tresholding: 0.2220035052\n",
      "\n",
      "Train and test on tournament\n",
      "Train seasons: [2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2597437042\n",
      "Brier Score after tresholding: 0.2607051521\n",
      "\n",
      "Train on regular season and test on tournament with no conferences\n",
      "Train seasons: [2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2537444326\n",
      "Brier Score after tresholding: 0.2500000000\n",
      "\n",
      "Train and test on regular season\n",
      "Train seasons: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2216529982\n",
      "Brier Score after tresholding: 0.2218423569\n",
      "\n",
      "Train on regular season and test on tournament\n",
      "Train seasons: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2264822128\n",
      "Brier Score after tresholding: 0.2260744202\n",
      "\n",
      "Train and test on tournament\n",
      "Train seasons: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2412839591\n",
      "Brier Score after tresholding: 0.2445695555\n",
      "\n",
      "Train on regular season and test on tournament with no conferences\n",
      "Train seasons: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2500268568\n",
      "Brier Score after tresholding: 0.2500000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train_seasons, test_seasons in zip(train_seasons_array, test_seasons_array):\n",
    "    \n",
    "    X_train, y_train, df_train = prepare_data(MRegularSeasonCompactResults, MTeamConferences, True, train_seasons)\n",
    "    X_test, y_test, df_test = prepare_data(MRegularSeasonCompactResults, MTeamConferences, True, test_seasons)\n",
    "    model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "    print(\"Train and test on regular season\")\n",
    "    print_tmp(train_seasons, test_seasons, y_test, y_pred_proba)\n",
    "\n",
    "    X_train, y_train, df_train = prepare_data(MRegularSeasonCompactResults, MTeamConferences, True, train_seasons)\n",
    "    X_test, y_test, df_test = prepare_data(MNCAATourneyCompactResults, MTeamConferences, True, test_seasons)\n",
    "    model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "    print(\"Train on regular season and test on tournament\")\n",
    "    print_tmp(train_seasons, test_seasons, y_test, y_pred_proba)\n",
    "\n",
    "    X_train, y_train, df_train = prepare_data(MNCAATourneyCompactResults, MTeamConferences, True, train_seasons)\n",
    "    X_test, y_test, df_test = prepare_data(MNCAATourneyCompactResults, MTeamConferences, True, test_seasons)\n",
    "    model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "    print(\"Train and test on tournament\")\n",
    "    print_tmp(train_seasons, test_seasons, y_test, y_pred_proba)\n",
    "\n",
    "    X_train, y_train, df_train = prepare_data(MRegularSeasonCompactResults, MTeamConferences, False, train_seasons)\n",
    "    X_test, y_test, df_test = prepare_data(MNCAATourneyCompactResults, MTeamConferences, False, test_seasons)\n",
    "    model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "    print(\"Train on regular season and test on tournament with no conferences\")\n",
    "    print_tmp(train_seasons, test_seasons, y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a4af1-0a6d-4933-98c8-078b8835cd9b",
   "metadata": {},
   "source": [
    "### On detailed data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2fb66abc-523a-4cc4-9432-02a9985b6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test on regular season\n",
      "Train seasons: [2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2220639657\n",
      "Brier Score after tresholding: 0.2222225503\n",
      "\n",
      "Train on regular season and test on tournament\n",
      "Train seasons: [2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2239554386\n",
      "Brier Score after tresholding: 0.2220035052\n",
      "\n",
      "Train and test on tournament\n",
      "Train seasons: [2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2597437042\n",
      "Brier Score after tresholding: 0.2607051521\n",
      "\n",
      "Train on regular season and test on tournament with no conferences\n",
      "Train seasons: [2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2537444326\n",
      "Brier Score after tresholding: 0.2500000000\n",
      "\n",
      "Train and test on regular season\n",
      "Train seasons: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2216529982\n",
      "Brier Score after tresholding: 0.2218423569\n",
      "\n",
      "Train on regular season and test on tournament\n",
      "Train seasons: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2264822128\n",
      "Brier Score after tresholding: 0.2260744202\n",
      "\n",
      "Train and test on tournament\n",
      "Train seasons: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2412839591\n",
      "Brier Score after tresholding: 0.2445695555\n",
      "\n",
      "Train on regular season and test on tournament with no conferences\n",
      "Train seasons: [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
      "Test seasons: [2023]\n",
      "Brier Score: 0.2500268568\n",
      "Brier Score after tresholding: 0.2500000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train_seasons, test_seasons in zip(train_seasons_array, test_seasons_array):\n",
    "    \n",
    "    X_train, y_train, df_train = prepare_data(MRegularSeasonDetailedResults, MTeamConferences, True, train_seasons)\n",
    "    X_test, y_test, df_test = prepare_data(MRegularSeasonDetailedResults, MTeamConferences, True, test_seasons)\n",
    "    model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "    print(\"Train and test on regular season\")\n",
    "    print_tmp(train_seasons, test_seasons, y_test, y_pred_proba)\n",
    "\n",
    "    X_train, y_train, df_train = prepare_data(MRegularSeasonDetailedResults, MTeamConferences, True, train_seasons)\n",
    "    X_test, y_test, df_test = prepare_data(MNCAATourneyDetailedResults, MTeamConferences, True, test_seasons)\n",
    "    model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "    print(\"Train on regular season and test on tournament\")\n",
    "    print_tmp(train_seasons, test_seasons, y_test, y_pred_proba)\n",
    "\n",
    "    X_train, y_train, df_train = prepare_data(MNCAATourneyDetailedResults, MTeamConferences, True, train_seasons)\n",
    "    X_test, y_test, df_test = prepare_data(MNCAATourneyDetailedResults, MTeamConferences, True, test_seasons)\n",
    "    model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "    print(\"Train and test on tournament\")\n",
    "    print_tmp(train_seasons, test_seasons, y_test, y_pred_proba)\n",
    "\n",
    "    X_train, y_train, df_train = prepare_data(MRegularSeasonDetailedResults, MTeamConferences, False, train_seasons)\n",
    "    X_test, y_test, df_test = prepare_data(MNCAATourneyDetailedResults, MTeamConferences, False, test_seasons)\n",
    "    model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "    print(\"Train on regular season and test on tournament with no conferences\")\n",
    "    print_tmp(train_seasons, test_seasons, y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70ebad-a946-4946-9826-286d622fcf5e",
   "metadata": {},
   "source": [
    "### Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2f8bdc4-5bac-421d-84dd-eb7939608ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 65\n",
      "Selected features: ['Loc_A', 'Loc_H', 'T1Conf_a_sun', 'T1Conf_a_ten', 'T1Conf_aac', 'T1Conf_acc', 'T1Conf_aec', 'T1Conf_big_east', 'T1Conf_big_sky', 'T1Conf_big_south', 'T1Conf_big_ten', 'T1Conf_big_twelve', 'T1Conf_big_west', 'T1Conf_caa', 'T1Conf_cusa', 'T1Conf_horizon', 'T1Conf_ind', 'T1Conf_ivy', 'T1Conf_maac', 'T1Conf_mac', 'T1Conf_meac', 'T1Conf_mvc', 'T1Conf_mwc', 'T1Conf_nec', 'T1Conf_ovc', 'T1Conf_pac_twelve', 'T1Conf_patriot', 'T1Conf_sec', 'T1Conf_southland', 'T1Conf_summit', 'T1Conf_swac', 'T1Conf_wac', 'T1Conf_wcc', 'T2Conf_a_sun', 'T2Conf_a_ten', 'T2Conf_aac', 'T2Conf_acc', 'T2Conf_aec', 'T2Conf_big_east', 'T2Conf_big_sky', 'T2Conf_big_south', 'T2Conf_big_ten', 'T2Conf_big_twelve', 'T2Conf_big_west', 'T2Conf_caa', 'T2Conf_cusa', 'T2Conf_horizon', 'T2Conf_ind', 'T2Conf_ivy', 'T2Conf_maac', 'T2Conf_mac', 'T2Conf_meac', 'T2Conf_mwc', 'T2Conf_nec', 'T2Conf_ovc', 'T2Conf_pac_twelve', 'T2Conf_patriot', 'T2Conf_sec', 'T2Conf_southern', 'T2Conf_southland', 'T2Conf_summit', 'T2Conf_sun_belt', 'T2Conf_swac', 'T2Conf_wac', 'T2Conf_wcc']\n",
      "Test set Brier Score: 0.2278188108456105\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Assume X is your DataFrame with features and y is your binary target (0 or 1)\n",
    "# For example:\n",
    "# X = pd.read_csv(\"your_features.csv\")\n",
    "# y = pd.read_csv(\"your_labels.csv\").squeeze()  # ensure y is a Series\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, y_train, df_train = prepare_data(MRegularSeasonCompactResults, MTeamConferences, True, [2022, 2023])\n",
    "X_test, y_test, df_test = prepare_data(MNCAATourneyCompactResults, MTeamConferences, True, [2024])\n",
    "model, y_pred_proba = logistic_regression(X_train, y_train, X_test)\n",
    "\n",
    "# Initialize logistic regression (using 'liblinear' is often good for smaller datasets and supports L1 penalty if needed)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create a scorer using the Brier score loss. Since lower is better,\n",
    "# we set greater_is_better=False so that the RFECV can maximize the (negative) score.\n",
    "scorer = make_scorer(brier_score_loss, greater_is_better=False)\n",
    "\n",
    "# Use RFECV to perform recursive feature elimination with cross-validation\n",
    "rfecv = RFECV(estimator=clf, step=1, cv=5, scoring=scorer)\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal number of features:\", rfecv.n_features_)\n",
    "print(\"Selected features:\", X_train.columns[rfecv.support_].tolist())\n",
    "\n",
    "# Retrain the logistic regression model using only the selected features\n",
    "clf.fit(X_train.loc[:, rfecv.support_], y_train)\n",
    "\n",
    "# Make probability predictions on the test set\n",
    "y_pred_proba = clf.predict_proba(X_test.loc[:, rfecv.support_])[:, 1]\n",
    "\n",
    "# Evaluate the model using the Brier score on the test set\n",
    "brier = brier_score_loss(y_test, y_pred_proba)\n",
    "print(\"Test set Brier Score:\", brier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90c3d00a-a8e7-47a0-bb0f-c393815881ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features (Random Forest): 1\n",
      "Selected features (Random Forest): ['Loc_A']\n",
      "Test set Brier Score (Random Forest): 0.24068708080640028\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Assume X is your DataFrame with features and y is your binary target (0 or 1)\n",
    "# For example:\n",
    "# X = pd.read_csv(\"your_features.csv\")\n",
    "# y = pd.read_csv(\"your_labels.csv\").squeeze()  # ensure y is a Series\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Create a scorer using the Brier score loss.\n",
    "# We use greater_is_better=False because lower Brier score indicates better performance.\n",
    "scorer = make_scorer(brier_score_loss, greater_is_better=False)\n",
    "\n",
    "# Use RFECV to perform recursive feature elimination with cross-validation\n",
    "rfecv_rf = RFECV(estimator=rf_model, step=1, cv=5, scoring=scorer)\n",
    "rfecv_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal number of features (Random Forest):\", rfecv_rf.n_features_)\n",
    "print(\"Selected features (Random Forest):\", X_train.columns[rfecv_rf.support_].tolist())\n",
    "\n",
    "# Retrain the Random Forest model using only the selected features\n",
    "rf_model.fit(X_train.loc[:, rfecv_rf.support_], y_train)\n",
    "\n",
    "# Make probability predictions on the test set\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test.loc[:, rfecv_rf.support_])[:, 1]\n",
    "\n",
    "# Evaluate the model using the Brier score on the test set\n",
    "brier_rf = brier_score_loss(y_test, y_pred_proba_rf)\n",
    "print(\"Test set Brier Score (Random Forest):\", brier_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32536a50-865e-4abe-9177-83db8c35e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV Brier Scores: [0.22838221065078593, 0.24019922013470363, 0.2224783155889698]\n",
      "Average Logistic Regression CV Brier Score: 0.23035324879148644\n",
      "Final Logistic Regression Test Brier Score: 0.25443847228100314\n",
      "Final Calibrated Random Forest Test Brier Score: 0.26487586527590296\n",
      "Submission from Logistic Regression:\n",
      "                  ID   Pred_lr\n",
      "1315  2024_1161_1438  0.474249\n",
      "1316  2024_1224_1447  0.493097\n",
      "1317  2024_1129_1160  0.485882\n",
      "1318  2024_1212_1286  0.496201\n",
      "1319  2024_1112_1253  0.484821\n",
      "Submission from Calibrated Random Forest:\n",
      "                  ID   Pred_rf\n",
      "1315  2024_1161_1438  0.466241\n",
      "1316  2024_1224_1447  0.525222\n",
      "1317  2024_1129_1160  0.421288\n",
      "1318  2024_1212_1286  0.514271\n",
      "1319  2024_1112_1253  0.576410\n"
     ]
    }
   ],
   "source": [
    "# --- Assume X_train, y_train, X_test, y_test, and df_train are already defined ---\n",
    "# df_train should include a 'Season' column. Make sure it’s sorted by Season.\n",
    "df_train_sorted = df_train.sort_values('Season')\n",
    "\n",
    "# Use TimeSeriesSplit for temporal (walk-forward) cross-validation.\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Logistic Regression (Baseline)\n",
    "# ------------------------------\n",
    "log_brier_scores = []\n",
    "for train_index, val_index in tscv.split(df_train_sorted):\n",
    "    X_train_cv = X_train.loc[df_train_sorted.index[train_index]]\n",
    "    y_train_cv = y_train.loc[df_train_sorted.index[train_index]]\n",
    "    X_val_cv = X_train.loc[df_train_sorted.index[val_index]]\n",
    "    y_val_cv = y_train.loc[df_train_sorted.index[val_index]]\n",
    "    \n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train_cv, y_train_cv)\n",
    "    y_val_pred_proba = lr.predict_proba(X_val_cv)[:, 1]\n",
    "    # Round extreme probabilities\n",
    "    y_val_pred_proba = np.where(y_val_pred_proba > 0.95, 1,\n",
    "                          np.where(y_val_pred_proba < 0.1, 0, y_val_pred_proba))\n",
    "    score = brier_score_loss(y_val_cv, y_val_pred_proba)\n",
    "    log_brier_scores.append(score)\n",
    "\n",
    "print(\"Logistic Regression CV Brier Scores:\", log_brier_scores)\n",
    "print(\"Average Logistic Regression CV Brier Score:\", np.mean(log_brier_scores))\n",
    "\n",
    "# Train final logistic regression on all training data\n",
    "final_lr = LogisticRegression(max_iter=1000)\n",
    "final_lr.fit(X_train, y_train)\n",
    "y_test_pred_lr = final_lr.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_lr = np.where(y_test_pred_lr > 0.95, 1,\n",
    "                    np.where(y_test_pred_lr < 0.1, 0, y_test_pred_lr))\n",
    "brier_lr = brier_score_loss(y_test, y_test_pred_lr)\n",
    "print(\"Final Logistic Regression Test Brier Score:\", brier_lr)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Tree-Based Model (Random Forest) with Calibration\n",
    "# ------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Calibrate using isotonic regression with the same temporal CV\n",
    "calibrated_rf = CalibratedClassifierCV(rf, method='isotonic', cv=tscv)\n",
    "calibrated_rf.fit(X_train, y_train)\n",
    "y_test_pred_rf = calibrated_rf.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_rf = np.where(y_test_pred_rf > 0.95, 1,\n",
    "                    np.where(y_test_pred_rf < 0.1, 0, y_test_pred_rf))\n",
    "brier_rf = brier_score_loss(y_test, y_test_pred_rf)\n",
    "print(\"Final Calibrated Random Forest Test Brier Score:\", brier_rf)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Prepare Final Predictions in Submission Format\n",
    "# ------------------------------\n",
    "# Assume df_test corresponds to the test set and contains Season, Team1, and Team2.\n",
    "# Create an ID in the format: Season_Team1_Team2.\n",
    "df_test['ID'] = df_test.apply(lambda row: f\"{row['Season']}_{row['Team1']}_{row['Team2']}\", axis=1)\n",
    "\n",
    "# Add predictions from both models to df_test.\n",
    "df_test['Pred_lr'] = y_test_pred_lr\n",
    "df_test['Pred_rf'] = y_test_pred_rf\n",
    "\n",
    "# Create submission DataFrames (example using logistic regression)\n",
    "submission_lr = df_test[['ID', 'Pred_lr']]\n",
    "submission_rf = df_test[['ID', 'Pred_rf']]\n",
    "\n",
    "print(\"Submission from Logistic Regression:\")\n",
    "print(submission_lr.head())\n",
    "\n",
    "print(\"Submission from Calibrated Random Forest:\")\n",
    "print(submission_rf.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da630569-b52a-473c-a7fd-e3d785ac26d9",
   "metadata": {},
   "source": [
    "## Another idea: make records with tams-vs-team pair scores\n",
    "Explored in First_model_team_approach notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
