{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Section 1 file: MTeams.csv and WTeams.csv\n",
    "\n",
    "\n",
    "These files identify the different college teams present in the dataset .\n",
    "\n",
    "    TeamID - a 4 digit id number, uniquely identifying each NCAAÂ® men's or women's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. The men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n",
    "    TeamName - a compact spelling of the team's college name, 16 characters or fewer.\n",
    "    FirstD1Season - the first season in our dataset that the school was a Division-I school. This column is only present in the men's data, so it is not found in WTeams.csv.\n",
    "    LastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2025. Again, this column is only present in the men's data, so it is not found in WTeams.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_teams = pd.read_csv(\"../../data/MTeams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_teams.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_scores_compact = pd.read_csv(\"../../data/MRegularSeasonCompactResults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_scores_compact.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_scores_compact.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_plays = men_scores_compact.drop(columns=['Season', 'DayNum','WScore', 'LScore', 'WLoc','NumOT' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_plays.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = men_plays.groupby(['WTeamID', 'LTeamID']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(occurrences['WTeamID'], occurrences['LTeamID'], s=occurrences['count'], alpha=0.5)\n",
    "plt.xlabel('WTeamID')\n",
    "plt.ylabel('LTeamID')\n",
    "plt.title('Occurrences of WTeamID vs LTeamID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/MRegularSeasonCompactResults.csv\")\n",
    "wins = df[['Season', 'DayNum', 'WTeamID']].copy()\n",
    "wins.columns = ['Season', 'DayNum', 'TeamID']\n",
    "wins['Result'] = 1\n",
    "losses = df[['Season', 'DayNum', 'LTeamID']].copy()\n",
    "losses.columns = ['Season', 'DayNum', 'TeamID']\n",
    "losses['Result'] = 0\n",
    "games = pd.concat([wins, losses], ignore_index=True)\n",
    "games = games.sort_values(['TeamID', 'Season', 'DayNum']).reset_index(drop=True)\n",
    "\n",
    "def compute_loss_streak(results):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    for r in results:\n",
    "        streaks.append(streak)\n",
    "        if r == 0:  \n",
    "            streak += 1\n",
    "        else:                   streak = 0\n",
    "    return streaks\n",
    "\n",
    "games['LossStreak'] = games.groupby('TeamID')['Result'].transform(compute_loss_streak)\n",
    "streak_summary = games.groupby('LossStreak')['Result'].agg(['mean', 'count']).reset_index()\n",
    "streak_summary.rename(columns={'mean': 'WinProbability', 'count': 'NumGames'}, inplace=True)\n",
    "print(streak_summary)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(streak_summary['LossStreak'], streak_summary['WinProbability'], marker='o')\n",
    "plt.xlabel(\"Loss Streak (games lost consecutively before the current game)\")\n",
    "plt.ylabel(\"Win Probability in the Current Game\")\n",
    "plt.title(\"Win Probability vs. Prior Loss Streak\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/MRegularSeasonCompactResults.csv')\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "df_non_neutral = df[df['WLoc'] != 'N'].copy()\n",
    "location_counts = df_non_neutral['WLoc'].value_counts()\n",
    "print(\"\\nCount of non-neutral games by winning location:\")\n",
    "print(location_counts)\n",
    "total_non_neutral = location_counts.sum()\n",
    "win_percentages = (location_counts / total_non_neutral * 100).round(2)\n",
    "print(\"\\nPercentage of wins by winning location:\")\n",
    "print(win_percentages)\n",
    "home_win_rate = win_percentages.get('H', 0)\n",
    "print(f\"\\nHome team win rate (non-neutral games): {home_win_rate:.2f}%\")\n",
    "df_non_neutral['Margin'] = df_non_neutral['WScore'] - df_non_neutral['LScore']\n",
    "margin_by_loc = df_non_neutral.groupby('WLoc')['Margin'].mean()\n",
    "print(\"\\nAverage margin of victory by winning location:\")\n",
    "print(margin_by_loc)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "location_counts.plot(kind='bar', ax=ax[0], color=['skyblue', 'salmon'])\n",
    "ax[0].set_title(\"Count of Non-Neutral Games\")\n",
    "ax[0].set_xlabel(\"Winning Location\\n(H = Home, A = Away)\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "win_percentages.plot(kind='bar', ax=ax[1], color=['skyblue', 'salmon'])\n",
    "ax[1].set_title(\"Win Percentages by Location\")\n",
    "ax[1].set_xlabel(\"Winning Location\\n(H = Home, A = Away)\")\n",
    "ax[1].set_ylabel(\"Percentage (%)\")\n",
    "margin_by_loc.plot(kind='bar', ax=ax[2], color=['skyblue', 'salmon'])\n",
    "ax[2].set_title(\"Average Margin of Victory\")\n",
    "ax[2].set_xlabel(\"Winning Location\\n(H = Home, A = Away)\")\n",
    "ax[2].set_ylabel(\"Average Margin\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/MRegularSeasonDetailedResults.csv')\n",
    "print(\"First 5 rows of the detailed results dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "df['W_Attempts'] = df['WFGA'] + 0.44 * df['WFTA']\n",
    "df['L_Attempts'] = df['LFGA'] + 0.44 * df['LFTA']\n",
    "print(\"\\nDescriptive statistics for winners' shot attempts:\")\n",
    "print(df['W_Attempts'].describe())\n",
    "print(\"\\nDescriptive statistics for losers' shot attempts:\")\n",
    "print(df['L_Attempts'].describe())\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "data = [df['W_Attempts'], df['L_Attempts']]\n",
    "ax.boxplot(data, labels=['Winners', 'Losers'])\n",
    "ax.set_title(\"Shot Attempts (Style of Play) for Winners vs. Losers\")\n",
    "ax.set_ylabel(\"Shot Attempts (FGA + 0.44*FTA)\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df['W_Attempts'], bins=50, alpha=0.6, label='Winners')\n",
    "plt.hist(df['L_Attempts'], bins=50, alpha=0.6, label='Losers')\n",
    "plt.title(\"Histogram of Shot Attempts for Winners vs. Losers\")\n",
    "plt.xlabel(\"Shot Attempts (FGA + 0.44*FTA)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "df['Margin'] = df['WScore'] - df['LScore']\n",
    "df['Attempt_Diff'] = df['W_Attempts'] - df['L_Attempts']\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['Attempt_Diff'], df['Margin'], alpha=0.5)\n",
    "plt.title(\"Difference in Shot Attempts vs. Margin of Victory\")\n",
    "plt.xlabel(\"Shot Attempts Difference (Winner - Loser)\")\n",
    "plt.ylabel(\"Margin of Victory\")\n",
    "plt.show()\n",
    "correlation = df['Attempt_Diff'].corr(df['Margin'])\n",
    "print(f\"\\nCorrelation between shot attempt difference and margin of victory: {correlation:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coaches = pd.read_csv('../../data/MTeamCoaches.csv')\n",
    "games = pd.read_csv('../../data/MRegularSeasonCompactResults.csv')\n",
    "\n",
    "winners = games[['Season', 'DayNum', 'WTeamID']].copy()\n",
    "winners.rename(columns={'WTeamID': 'TeamID'}, inplace=True)\n",
    "winners['Outcome'] = 1  \n",
    "\n",
    "losers = games[['Season', 'DayNum', 'LTeamID']].copy()\n",
    "losers.rename(columns={'LTeamID': 'TeamID'}, inplace=True)\n",
    "losers['Outcome'] = 0  \n",
    "\n",
    "coach_games = pd.concat([winners, losers], ignore_index=True)\n",
    "merged = pd.merge(coach_games, coaches, on=['Season', 'TeamID'], how='left')\n",
    "merged = merged[(merged['DayNum'] >= merged['FirstDayNum']) & (merged['DayNum'] <= merged['LastDayNum'])]\n",
    "coach_performance = merged.groupby('CoachName').agg(\n",
    "    games_coached=('Outcome', 'count'),\n",
    "    wins=('Outcome', 'sum')\n",
    ").reset_index()\n",
    "coach_performance['win_pct'] = coach_performance['wins'] / coach_performance['games_coached']\n",
    "min_games = 50\n",
    "coach_performance_filtered = coach_performance[coach_performance['games_coached'] >= min_games]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(coach_performance_filtered['win_pct'], bins=20, kde=True)\n",
    "plt.xlabel('Win Percentage')\n",
    "plt.title(f'Distribution of Coach Win Percentages (Coaches with >= {min_games} Games)')\n",
    "plt.show()\n",
    "\n",
    "first_season = merged.groupby('CoachName')['Season'].min().reset_index().rename(columns={'Season': 'first_season'})\n",
    "merged = pd.merge(merged, first_season, on='CoachName', how='left')\n",
    "merged['is_first_season'] = np.where(merged['Season'] == merged['first_season'], 'First Season', 'Subsequent Seasons')\n",
    "first_season_stats = merged.groupby('is_first_season').agg(\n",
    "    games=('Outcome', 'count'),\n",
    "    wins=('Outcome', 'sum')\n",
    ").reset_index()\n",
    "first_season_stats['win_pct'] = first_season_stats['wins'] / first_season_stats['games']\n",
    "print(\"Win percentages by season type:\")\n",
    "print(first_season_stats)\n",
    "coach_season_win = merged.groupby(['CoachName', 'is_first_season']).agg(\n",
    "    games_coached=('Outcome', 'count'),\n",
    "    wins=('Outcome', 'sum')\n",
    ").reset_index()\n",
    "coach_season_win['win_pct'] = coach_season_win['wins'] / coach_season_win['games_coached']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='is_first_season', y='win_pct', data=coach_season_win)\n",
    "plt.xlabel('Season Type')\n",
    "plt.ylabel('Win Percentage')\n",
    "plt.title('Coach Win Percentage: First Season vs. Subsequent Seasons')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.read_csv('../../data/MRegularSeasonCompactResults.csv')\n",
    "teams_df = pd.read_csv('../../data/MTeams.csv')\n",
    "\n",
    "team_names = dict(zip(teams_df['TeamID'], teams_df['TeamName']))\n",
    "\n",
    "options_teamA = [(team_names[tid], tid) for tid in teams_df['TeamID']]\n",
    "options_teamA = sorted(options_teamA, key=lambda x: x[0])\n",
    "dropdown_teamA = widgets.Dropdown(options=options_teamA, description='Team A:')\n",
    "\n",
    "dropdown_teamB = widgets.Dropdown(options=[], description='Team B:')\n",
    "def update_teamB_options(change):\n",
    "    teamA = change['new']\n",
    "    \n",
    "    subset = games[(games['WTeamID'] == teamA) | (games['LTeamID'] == teamA)]\n",
    "    opponents = set(subset['WTeamID'].tolist() + subset['LTeamID'].tolist())\n",
    "    opponents.discard(teamA)\n",
    "    new_options = [(team_names[tid], tid) for tid in opponents]\n",
    "    new_options = sorted(new_options, key=lambda x: x[0])\n",
    "    dropdown_teamB.options = new_options\n",
    "    if new_options:\n",
    "        dropdown_teamB.value = new_options[0][1]\n",
    "    else:\n",
    "        dropdown_teamB.value = None\n",
    "dropdown_teamA.observe(update_teamB_options, names='value')\n",
    "\n",
    "update_teamB_options({'new': dropdown_teamA.value})\n",
    "def update_plot(teamA, teamB):\n",
    "    clear_output(wait=True)\n",
    "    display(interactive_plot)\n",
    "    \n",
    "    \n",
    "\n",
    "    subset = games[\n",
    "        ((games['WTeamID'] == teamA) & (games['LTeamID'] == teamB)) |\n",
    "        ((games['WTeamID'] == teamB) & (games['LTeamID'] == teamA))\n",
    "    ]\n",
    "    \n",
    "    if subset.empty:\n",
    "        print(\"No games found between {} and {}.\".format(team_names[teamA], team_names[teamB]))\n",
    "        return\n",
    "    \n",
    "\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=len(subset), description='Processing:')\n",
    "    display(progress)\n",
    " \n",
    "   \n",
    "\n",
    "    margins = []\n",
    "    for _, row in subset.iterrows():\n",
    "        if row['WTeamID'] == teamA:\n",
    "            margin = row['WScore'] - row['LScore']\n",
    "        else:\n",
    "            margin = -(row['LScore'] - row['WScore'])\n",
    "        margins.append(margin)\n",
    "        progress.value += 1  \n",
    "    \n",
    "\n",
    "    subset = subset.copy()\n",
    "    subset['margin'] = margins\n",
    "    \n",
    "    \n",
    "\n",
    "    season_net = subset.groupby('Season')['margin'].sum().reset_index().sort_values('Season')\n",
    "    \n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(season_net['Season'], season_net['margin'], marker='o', linestyle='-')\n",
    "    plt.xlabel('Season')\n",
    "    plt.ylabel('Net Margin for {}'.format(team_names[teamA]))\n",
    "    plt.title('Net Margin Over Time: {} vs {}'.format(team_names[teamA], team_names[teamB]))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "    progress.layout.visibility = 'hidden'\n",
    "interactive_plot = widgets.interactive(update_plot, teamA=dropdown_teamA, teamB=dropdown_teamB)\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "games = pd.read_csv('../../data/MRegularSeasonCompactResults.csv')\n",
    "teams = pd.read_csv('../../data/MTeams.csv')\n",
    "team_names = dict(zip(teams['TeamID'], teams['TeamName']))\n",
    "\n",
    "games['TeamA'] = games[['WTeamID', 'LTeamID']].min(axis=1)\n",
    "games['TeamB'] = games[['WTeamID', 'LTeamID']].max(axis=1)\n",
    "\n",
    "pair_counts = games.groupby(['TeamA', 'TeamB']).size().reset_index(name='games_played')\n",
    "top_pairs = pair_counts.sort_values('games_played', ascending=False).head(10)\n",
    "top_pairs['TeamAName'] = top_pairs['TeamA'].map(team_names)\n",
    "top_pairs['TeamBName'] = top_pairs['TeamB'].map(team_names)\n",
    "print(\"Top 10 pairs of teams that have played the most games together:\")\n",
    "print(top_pairs[['TeamAName', 'TeamBName', 'games_played']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "games = pd.read_csv('../../data/MRegularSeasonCompactResults.csv')\n",
    "games['TeamA'] = games[['WTeamID', 'LTeamID']].min(axis=1)\n",
    "games['TeamB'] = games[['WTeamID', 'LTeamID']].max(axis=1)\n",
    "def split_train_test_by_pair(games_df, n=1):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    grouped = games_df.groupby(['TeamA', 'TeamB'])\n",
    "    \n",
    "    for (teamA, teamB), group in grouped:\n",
    "        group_sorted = group.sort_values(['Season', 'DayNum'])\n",
    "        if len(group_sorted) > n:\n",
    "            train = group_sorted.iloc[:-n]\n",
    "            test = group_sorted.iloc[-n:]\n",
    "        else:\n",
    "            train = group_sorted.iloc[0:0]\n",
    "            test = group_sorted\n",
    "        \n",
    "        train_list.append(train)\n",
    "        test_list.append(test)\n",
    "    \n",
    "    train_set = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_set = pd.concat(test_list).reset_index(drop=True)\n",
    "    \n",
    "    return train_set, test_set\n",
    "\n",
    "n = 1\n",
    "\n",
    "train_set, test_set = split_train_test_by_pair(games, n=n)\n",
    "\n",
    "print(\"Train set shape:\", train_set.shape)\n",
    "print(\"Test set shape:\", test_set.shape)\n",
    "print(test_set.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1  \n",
    "train_set, test_set = split_train_test_by_pair(games, n=n)\n",
    "\n",
    "print(\"Train set shape:\", train_set.shape)\n",
    "print(\"Test set shape:\", test_set.shape)\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['Outcome'] = (df['WTeamID'] == df['TeamA']).astype(int)\n",
    "    X = df[['TeamA', 'TeamB']].astype(str)\n",
    "    y = df['Outcome']\n",
    "    X_encoded = pd.get_dummies(X, columns=['TeamA', 'TeamB'])\n",
    "    return X_encoded, y\n",
    "\n",
    "X_train, y_train = create_features(train_set)\n",
    "X_test, y_test = create_features(test_set)\n",
    "\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "models = {\n",
    "    'XGBoost (GPU)': xgb.XGBClassifier(tree_method='gpu_hist', use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM (GPU)': lgb.LGBMClassifier(device='gpu'),\n",
    "    'Logistic Regression (CPU)': LogisticRegression(max_iter=1000),\n",
    "}\n",
    "\n",
    "bootstrap_threshold = 200 \n",
    "bootstrap_factor = 3\n",
    "\n",
    "progress = widgets.IntProgress(value=0, min=0, max=len(models), description='Models:')\n",
    "display(progress)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if len(X_train) < bootstrap_threshold:\n",
    "        X_train_boot = X_train.sample(n=bootstrap_factor * len(X_train), replace=True, random_state=42)\n",
    "        y_train_boot = y_train.loc[X_train_boot.index]\n",
    "        X_to_train = X_train_boot\n",
    "        y_to_train = y_train_boot\n",
    "    else:\n",
    "        X_to_train = X_train\n",
    "        y_to_train = y_train\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(X_to_train, y_to_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.decision_function(X_test)\n",
    "        y_pred_proba = 1 / (1 + np.exp(-y_pred_proba))\n",
    "        \n",
    "    brier = brier_score_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "         'model': model,\n",
    "         'training_time': training_time,\n",
    "         'brier_score': brier,\n",
    "         'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    progress.value += 1\n",
    "\n",
    "progress.layout.visibility = 'hidden'\n",
    "\n",
    "print(\"Trained models and their performance on the test set:\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name} - Training time: {res['training_time']:.2f} sec, Brier Score: {res['brier_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for logistic regression\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],  # saga supports both l1 and l2\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "# Initialize the logistic regression model (CPU-based)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Set up GridSearchCV with cross-validation and parallel processing\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=logreg,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,            # Use all CPU cores\n",
    "    scoring='neg_log_loss'  # or another scoring metric like 'accuracy'\n",
    ")\n",
    "\n",
    "# Fit grid search on your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and best model\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "best_logreg = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "weight_decays = [0, 1e-4, 1e-3, 1e-2]\n",
    "param_grid = [(lr, wd) for lr in learning_rates for wd in weight_decays]\n",
    "n_total = len(param_grid)\n",
    "\n",
    "progress = widgets.IntProgress(value=0, min=0, max=n_total, description='GPU Tuning:')\n",
    "display(progress)\n",
    "\n",
    "best_brier = float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "num_epochs = 50  \n",
    "batch_size = 64\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.BCELoss()  \n",
    "\n",
    "\n",
    "for i, (lr, wd) in enumerate(param_grid):\n",
    "    \n",
    "    model = LogisticRegressionModel(input_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        \n",
    "        brier_score = torch.mean((y_pred - y_test_tensor) ** 2).item()\n",
    "    \n",
    "    \n",
    "    print(f\"LR: {lr}, Weight Decay: {wd}, Brier Score: {brier_score:.4f}\")\n",
    "    \n",
    "    \n",
    "    if brier_score < best_brier:\n",
    "        best_brier = brier_score\n",
    "        best_params = {'learning_rate': lr, 'weight_decay': wd}\n",
    "        best_model = model\n",
    "    \n",
    "    \n",
    "    progress.value = i + 1\n",
    "\n",
    "print(\"Best GPU (PyTorch) parameters:\", best_params)\n",
    "print(\"Best GPU (PyTorch) Brier score:\", best_brier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best GPU (PyTorch) parameters: {'learning_rate': 0.001, 'weight_decay': 0}\n",
    "Best GPU (PyTorch) Brier score: 0.17488974332809448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# (Assumes the following helper functions are defined from your earlier code)\n",
    "# split_train_test_by_pair(games, n) and create_features(df)\n",
    "\n",
    "# For example, using n = 1:\n",
    "n = 1  \n",
    "train_set, test_set = split_train_test_by_pair(games, n=n)\n",
    "print(\"Train set shape:\", train_set.shape)\n",
    "print(\"Test set shape:\", test_set.shape)\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['Outcome'] = (df['WTeamID'] == df['TeamA']).astype(int)\n",
    "    X = df[['TeamA', 'TeamB']].astype(str)\n",
    "    y = df['Outcome']\n",
    "    X_encoded = pd.get_dummies(X, columns=['TeamA', 'TeamB'])\n",
    "    return X_encoded, y\n",
    "\n",
    "X_train, y_train = create_features(train_set)\n",
    "X_test, y_test = create_features(test_set)\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# If the training set is small, force the models to run on CPU.\n",
    "if X_train.shape[0] < 1000:\n",
    "    print(\"Dataset is small; using CPU for tree-based models.\")\n",
    "    models = {\n",
    "         'XGBoost (CPU)': xgb.XGBClassifier(tree_method='hist', \n",
    "                                            use_label_encoder=False, \n",
    "                                            eval_metric='logloss'),\n",
    "         'LightGBM (CPU)': lgb.LGBMClassifier(device='cpu'),\n",
    "         'Logistic Regression (CPU)': LogisticRegression(max_iter=1000),\n",
    "    }\n",
    "else:\n",
    "    models = {\n",
    "         'XGBoost (GPU)': xgb.XGBClassifier(tree_method='gpu_hist', \n",
    "                                            use_label_encoder=False, \n",
    "                                            eval_metric='logloss'),\n",
    "         'LightGBM (GPU)': lgb.LGBMClassifier(device='gpu'),\n",
    "         'Logistic Regression (CPU)': LogisticRegression(max_iter=1000),\n",
    "    }\n",
    "\n",
    "bootstrap_threshold = 200 \n",
    "bootstrap_factor = 3\n",
    "\n",
    "progress = widgets.IntProgress(value=0, min=0, max=len(models), description='Models:')\n",
    "display(progress)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Optionally use bootstrap sampling if training set is very small.\n",
    "    if len(X_train) < bootstrap_threshold:\n",
    "        X_train_boot = X_train.sample(n=bootstrap_factor * len(X_train), replace=True, random_state=42)\n",
    "        y_train_boot = y_train.loc[X_train_boot.index]\n",
    "        X_to_train = X_train_boot\n",
    "        y_to_train = y_train_boot\n",
    "    else:\n",
    "        X_to_train = X_train\n",
    "        y_to_train = y_train\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(X_to_train, y_to_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.decision_function(X_test)\n",
    "        y_pred_proba = 1 / (1 + np.exp(-y_pred_proba))\n",
    "        \n",
    "    brier = brier_score_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "         'model': model,\n",
    "         'training_time': training_time,\n",
    "         'brier_score': brier,\n",
    "         'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    progress.value += 1\n",
    "\n",
    "progress.layout.visibility = 'hidden'\n",
    "\n",
    "print(\"Trained models and their performance on the test set:\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name} - Training time: {res['training_time']:.2f} sec, Brier Score: {res['brier_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More advanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_year(games_df, predict_year): # useful for predicting future seasons\n",
    "    train_set = games_df[games_df['Season'] < predict_year]\n",
    "    test_set = games_df[games_df['Season'] == predict_year]\n",
    "    return train_set, test_set\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['Outcome'] = (df['WTeamID'] == df['TeamA']).astype(int)\n",
    "    X = df[['TeamA', 'TeamB']].astype(str)\n",
    "    y = df['Outcome']\n",
    "    X_encoded = pd.get_dummies(X, columns=['TeamA', 'TeamB'])\n",
    "    return X_encoded, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pd.read_csv('../../data/MRegularSeasonCompactResults.csv')\n",
    "teams = pd.read_csv('../../data/MTeams.csv')\n",
    "\n",
    "team_names = dict(zip(teams['TeamID'], teams['TeamName']))\n",
    "\n",
    "games['TeamA'] = games[['WTeamID', 'LTeamID']].min(axis=1)\n",
    "games['TeamB'] = games[['WTeamID', 'LTeamID']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_year = 2024\n",
    "train_set, test_set = split_by_year(games, predict_year)\n",
    "print(\"Train set shape:\", train_set.shape)\n",
    "print(\"Test set shape:\", test_set.shape)\n",
    "\n",
    "X_train, y_train = create_features(train_set)\n",
    "X_test, y_test = create_features(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "param_combinations = [{'C': c} for c in hyperparameter_grid['C']]\n",
    "n_total = len(param_combinations)\n",
    "print(\"Total hyperparameter combinations:\", n_total)\n",
    "\n",
    "\n",
    "progress = widgets.IntProgress(value=0, min=0, max=n_total, description='Tuning:')\n",
    "display(progress)\n",
    "\n",
    "best_brier = float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    model = LogisticRegression(C=params['C'], max_iter=10000)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Predict probabilities on the test set.\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    brier = brier_score_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"Params: {params}, Training time: {training_time:.2f} sec, Brier Score: {brier:.4f}\")\n",
    "    \n",
    "    if brier < best_brier:\n",
    "        best_brier = brier\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "    \n",
    "    progress.value = i + 1\n",
    "\n",
    "progress.layout.visibility = 'hidden'\n",
    "\n",
    "print(\"\\nBest parameters:\", best_params)\n",
    "print(\"Best Brier Score:\", best_brier)\n",
    "\n",
    "# -------------------------------\n",
    "# Saving the Best Model & Description\n",
    "# -------------------------------\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "model_filename = os.path.join(\"saved_models\", \"best_logistic_regression_model\" + str(predict_year) + \".pkl\")\n",
    "desc_filename = os.path.join(\"saved_models\", \"best_logistic_regression_model\" + str(predict_year) + \".json\")\n",
    "\n",
    "# Save the model using joblib.\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "# Save the description.\n",
    "description = {\n",
    "    \"model_type\": \"Logistic Regression\",\n",
    "    \"hyperparameters\": best_params,\n",
    "    \"brier_score\": best_brier,\n",
    "    \"predict_year\": predict_year,\n",
    "    \"train_set_shape\": train_set.shape,\n",
    "    \"test_set_shape\": test_set.shape\n",
    "}\n",
    "with open(desc_filename, \"w\") as f:\n",
    "    json.dump(description, f, indent=4)\n",
    "\n",
    "print(f\"Best model saved to {model_filename}\")\n",
    "print(f\"Model description saved to {desc_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# -------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# -------------------------------\n",
    "\n",
    "# Load game results (assumed to be in ../../data/)\n",
    "games = pd.read_csv('../../data/MRegularSeasonCompactResults.csv')\n",
    "\n",
    "# Create unique matchup identifiers: TeamA is the lower ID, TeamB is the higher ID.\n",
    "games['TeamA'] = games[['WTeamID', 'LTeamID']].min(axis=1)\n",
    "games['TeamB'] = games[['WTeamID', 'LTeamID']].max(axis=1)\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of games, creates one-hot encoded features for team matchups and the target.\n",
    "    Outcome is 1 if the lower-ID team (TeamA) wins (i.e. if WTeamID equals TeamA), else 0.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['Outcome'] = (df['WTeamID'] == df['TeamA']).astype(int)\n",
    "    # Convert team IDs to strings so that one-hot encoding creates categorical features.\n",
    "    X = df[['TeamA', 'TeamB']].astype(str)\n",
    "    y = df['Outcome']\n",
    "    X_encoded = pd.get_dummies(X, columns=['TeamA', 'TeamB'])\n",
    "    return X_encoded, y\n",
    "\n",
    "def train_and_predict(test_year, min_train_year=2013):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model using all games from min_train_year up to test_year - 1,\n",
    "    then predicts on games from test_year.\n",
    "    \n",
    "    Parameters:\n",
    "      test_year: The year to predict.\n",
    "      min_train_year: The earliest season to use for training.\n",
    "    \n",
    "    Returns:\n",
    "      model: Trained LogisticRegression model.\n",
    "      brier: Brier score on the test set.\n",
    "      X_test: Features for the test set.\n",
    "      y_test: True outcomes for the test set.\n",
    "      y_pred: Predicted probabilities for the test set.\n",
    "    \"\"\"\n",
    "    # Training set: all games with Season >= min_train_year and Season < test_year.\n",
    "    train_set = games[(games['Season'] >= min_train_year) & (games['Season'] < test_year)]\n",
    "    # Test set: games with Season == test_year.\n",
    "    test_set = games[games['Season'] == test_year]\n",
    "    \n",
    "    # Create features.\n",
    "    X_train, y_train = create_features(train_set)\n",
    "    X_test, y_test = create_features(test_set)\n",
    "    \n",
    "    # Align features so that both training and test sets have the same dummy columns.\n",
    "    X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "    \n",
    "    # Train logistic regression.\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities.\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute Brier score.\n",
    "    brier = brier_score_loss(y_test, y_pred)\n",
    "    \n",
    "    return model, brier, X_test, y_test, y_pred\n",
    "\n",
    "# -------------------------------\n",
    "# Rolling Forecast: Loop Over Years\n",
    "# -------------------------------\n",
    "\n",
    "# We'll make predictions for each year from 2014 to 2024.\n",
    "years = list(range(2014, 2025))\n",
    "results = {}\n",
    "\n",
    "# Create a progress bar.\n",
    "progress = widgets.IntProgress(value=0, min=0, max=len(years), description='Rolling Forecast:')\n",
    "display(progress)\n",
    "\n",
    "for i, yr in enumerate(years):\n",
    "    model, brier, X_test, y_test, y_pred = train_and_predict(test_year=yr, min_train_year=2013)\n",
    "    results[yr] = {\n",
    "        'model': model,\n",
    "        'brier': brier,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    progress.value = i + 1\n",
    "\n",
    "progress.layout.visibility = 'hidden'\n",
    "\n",
    "# -------------------------------\n",
    "# Reporting the Results\n",
    "# -------------------------------\n",
    "print(\"Rolling Forecast Performance (Brier Scores):\")\n",
    "for yr in years:\n",
    "    print(f\"Year: {yr}, Brier Score: {results[yr]['brier']:.4f}\")\n",
    "\n",
    "print(\"\\nFinal evaluation on 2024:\")\n",
    "print(f\"Brier Score for 2024: {results[2024]['brier']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
